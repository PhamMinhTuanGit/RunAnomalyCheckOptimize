{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5668389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23fca3f",
   "metadata": {},
   "source": [
    "# Pipeline ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfd4ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_excel_file_every_5seconds(file_path):\n",
    "    # Read the Excel file\n",
    "    xls = pd.read_excel(file_path, header=3)\n",
    "    for line in xls.iterrows():\n",
    "        \n",
    "        print(line)\n",
    "        time.sleep(5)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        break  \n",
    "    return line  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8552fbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y                        ds unique_id  rolling_mean  rolling_std  \\\n",
      "24  43.0 2025-05-09 02:00:00+00:00  series_1     58.857143    11.596387   \n",
      "25  43.0 2025-05-09 02:05:00+00:00  series_1     54.857143    11.538755   \n",
      "26  44.0 2025-05-09 02:10:00+00:00  series_1     51.142857     9.923517   \n",
      "27  44.0 2025-05-09 02:15:00+00:00  series_1     48.714286     9.159954   \n",
      "28  42.0 2025-05-09 02:20:00+00:00  series_1     46.142857     7.904188   \n",
      "\n",
      "    lag_5min  lag_30min  lag_2h  \n",
      "24      43.0       71.0   103.0  \n",
      "25      43.0       70.0    96.0  \n",
      "26      43.0       61.0   100.0  \n",
      "27      44.0       60.0    89.0  \n",
      "28      44.0       64.0    82.0  \n"
     ]
    }
   ],
   "source": [
    "from src.utils.train_utils import load_and_process_data\n",
    "\n",
    "\n",
    "data_path = '/Users/phamminhtuan/Desktop/VHT_work/MLOps/data/raw/train'\n",
    "df = load_and_process_data(data_path, exog = True)\n",
    "df['ds'] = pd.to_datetime(df['ds'], utc = True)\n",
    "print(df.head())\n",
    "df.to_parquet(\"feature_store/feature_repo/data/train_data.parquet\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ab42219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [unique_id, event_timestamp, y, lag_2h, rolling_mean, rolling_std, lag_5min, lag_30min]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "\n",
    "# 1ï¸âƒ£ Entity dataframe (UTC)\n",
    "entity_df = pd.DataFrame({\n",
    "    \"unique_id\": [\"series_1\"],\n",
    "    \"event_timestamp\": [\"2025-06-09 00:15:00\"]\n",
    "})\n",
    "entity_df[\"event_timestamp\"] = pd.to_datetime(entity_df[\"event_timestamp\"])\n",
    "# 2ï¸âƒ£ Khá»Ÿi táº¡o store\n",
    "store = FeatureStore(repo_path=\"feature_store/feature_repo\")\n",
    "\n",
    "# 3ï¸âƒ£ Láº¥y historical features\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"interface_traffic:y\",\n",
    "        \"interface_traffic:lag_2h\",\n",
    "        \"interface_traffic:rolling_mean\",\n",
    "        \"interface_traffic:rolling_std\",\n",
    "        \"interface_traffic:lag_5min\",\n",
    "        \"interface_traffic:lag_30min\",\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecdcc37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y                   float64\n",
      "ds           datetime64[ns]\n",
      "unique_id            object\n",
      "dtype: object\n",
      "unique_id                  object\n",
      "event_timestamp    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/processed/train_data.parquet\")\n",
    "# print(df.head())\n",
    "print(df.dtypes)\n",
    "# print(df[\"unique_id\"].unique())\n",
    "# print(df[\"ds\"].min(), \"â†’\", df[\"ds\"].max())\n",
    "# print(df.head())\n",
    "print(entity_df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b17f30d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded source data:\n",
      "Unique IDs: ['series_1']\n",
      "Time range: 2025-05-09 02:00:00+00:00 â†’ 2025-06-08 23:55:00+00:00\n",
      "dtype: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_store/feature_repo\")\n",
    "\n",
    "# ğŸ”¹ Äá»c láº¡i source theo Ä‘Æ°á»ng dáº«n trong feature_repo\n",
    "source_path = \"feature_store/feature_repo/data/train_data.parquet\"\n",
    "\n",
    "offline_df = pd.read_parquet(source_path)\n",
    "print(\"âœ… Loaded source data:\")\n",
    "print(\"Unique IDs:\", offline_df[\"unique_id\"].unique())\n",
    "print(\"Time range:\", offline_df[\"ds\"].min(), \"â†’\", offline_df[\"ds\"].max())\n",
    "print(\"dtype:\", offline_df[\"ds\"].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8ae8c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  unique_id           event_timestamp      y  lag_2h  rolling_mean  \\\n",
      "0  series_1 2025-06-07 23:55:00+00:00  129.0   324.0    139.428571   \n",
      "\n",
      "   rolling_std  lag_5min  lag_30min  \n",
      "0     12.54136     146.0      152.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_store/feature_repo\")\n",
    "\n",
    "# ğŸ”¹ Láº¥y timestamp náº±m trong range\n",
    "entity_df = pd.DataFrame({\n",
    "    \"unique_id\": [\"series_1\"],\n",
    "    \"event_timestamp\": pd.to_datetime([\"2025-06-07 23:55:00\"], utc=True)\n",
    "})\n",
    "\n",
    "# ğŸ”¹ Láº¥y historical features\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"interface_traffic:y\",\n",
    "        \"interface_traffic:lag_2h\",\n",
    "        \"interface_traffic:rolling_mean\",\n",
    "        \"interface_traffic:rolling_std\",\n",
    "        \"interface_traffic:lag_5min\",\n",
    "        \"interface_traffic:lag_30min\",\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "print(training_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef8c8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: data/train_data.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     49\u001b[39m     training_df = store.get_historical_features(\n\u001b[32m     50\u001b[39m         entity_df=entity_df,\n\u001b[32m     51\u001b[39m         features=features\n\u001b[32m     52\u001b[39m     ).to_df()\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m training_df\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m training_df = \u001b[43mget_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature_store/feature_repo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_view_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface_traffic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface_traffic:y\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface_traffic:lag_2h\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface_traffic:rolling_mean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface_traffic:rolling_std\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface_traffic:lag_5min\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface_traffic:lag_30min\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(training_df.head())\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(training_df.shape)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mget_training_data\u001b[39m\u001b[34m(repo_path, feature_view_name, features, entity_col, timestamp_col)\u001b[39m\n\u001b[32m     20\u001b[39m file_path = source.path  \u001b[38;5;66;03m# Ä‘Æ°á»ng dáº«n tá»›i file parquet/csv\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(file_path):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# ğŸ”¹ 2. Äá»c dá»¯ liá»‡u gá»‘c (tÃ¹y Ä‘á»‹nh dáº¡ng)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_path.endswith(\u001b[33m\"\u001b[39m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: data/train_data.parquet"
     ]
    }
   ],
   "source": [
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_training_data(\n",
    "    repo_path: str,\n",
    "    feature_view_name: str,\n",
    "    features: list = None,\n",
    "    entity_col: str = \"unique_id\",\n",
    "    timestamp_col: str = \"ds\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Láº¥y toÃ n bá»™ historical features tá»« Feast Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh.\n",
    "    \"\"\"\n",
    "\n",
    "    store = FeatureStore(repo_path=repo_path)\n",
    "\n",
    "    # ğŸ”¹ 1. Láº¥y source object vÃ  Ä‘Æ°á»ng dáº«n file\n",
    "    source = store.get_data_source(f\"{feature_view_name}_source\")\n",
    "    file_path = 'feature_store/feature_repo/' + source.path  # Ä‘Æ°á»ng dáº«n tá»›i file parquet/csv\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {file_path}\")\n",
    "\n",
    "    # ğŸ”¹ 2. Äá»c dá»¯ liá»‡u gá»‘c (tÃ¹y Ä‘á»‹nh dáº¡ng)\n",
    "    if file_path.endswith(\".parquet\"):\n",
    "        df_source = pd.read_parquet(file_path)\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        df_source = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Chá»‰ há»— trá»£ parquet hoáº·c csv trong vÃ­ dá»¥ nÃ y.\")\n",
    "\n",
    "    # ğŸ”¹ 3. Chuáº©n hoÃ¡ thá»i gian UTC\n",
    "    df_source[timestamp_col] = pd.to_datetime(df_source[timestamp_col], utc=True)\n",
    "\n",
    "    # ğŸ”¹ 4. Táº¡o entity_df Ä‘á»ƒ láº¥y historical feature\n",
    "    entity_df = df_source[[entity_col, timestamp_col]].rename(\n",
    "        columns={timestamp_col: \"event_timestamp\"}\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ 5. Náº¿u chÆ°a truyá»n danh sÃ¡ch feature, tá»± Ä‘á»™ng láº¥y táº¥t cáº£ feature trong view\n",
    "    if features is None:\n",
    "        features = [\n",
    "            f\"{feature_view_name}:{f.name}\"\n",
    "            for f in store.get_feature_view(feature_view_name).features\n",
    "        ]\n",
    "\n",
    "    # ğŸ”¹ 6. Láº¥y historical features tá»« Feast\n",
    "    training_df = store.get_historical_features(\n",
    "        entity_df=entity_df,\n",
    "        features=features\n",
    "    ).to_df()\n",
    "\n",
    "    return training_df\n",
    "training_df = get_training_data(\n",
    "    repo_path=\"feature_store/feature_repo\",\n",
    "    feature_view_name=\"interface_traffic\",\n",
    "    features=[\n",
    "        \"interface_traffic:y\",\n",
    "        \"interface_traffic:lag_2h\",\n",
    "        \"interface_traffic:rolling_mean\",\n",
    "        \"interface_traffic:rolling_std\",\n",
    "        \"interface_traffic:lag_5min\",\n",
    "        \"interface_traffic:lag_30min\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(training_df.head())\n",
    "print(training_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
